{"cells": [{"metadata": {"collapsed": true, "id": "bf2c379a-2252-45ae-80df-3e3cf5302c34"}, "cell_type": "markdown", "source": "## Toczala - CP4D Python Helper Routines"}, {"metadata": {"id": "80fdc8a41e8a4e6d8779687abcd0a615"}, "cell_type": "code", "source": "# Import needed Python packages\n\nimport os, uuid, sys\nimport pandas as pd\nimport requests\nimport json\nimport shutil\nimport warnings\nimport urllib3\nwarnings.simplefilter(action='ignore', category=urllib3.exceptions.InsecureRequestWarning)\n\nfrom project_lib import Project\nfrom ibm_watson_studio_lib import access_project_or_space\n\n#\n# YOU NEED TO SET THIS FOR EACH SPECIFIC PROJECT\n#\n# Set your Cloud Pak base access point\n#\nCP4D_BASE_URL = \"https://cpd-cpd-instance.apps.poc.watson.techops.aa.com\"\n#\n# Grab a Watson Studio lib object\n#\nwslib = access_project_or_space()\n#\n# Set/get platform and project info and token\n#\ntoken = os.environ.get('USER_ACCESS_TOKEN')\nproject_id = os.environ.get('PROJECT_ID')\nhost = CP4D_BASE_URL\n#\n# Set your REST message header (with authorization token)\n#\nheaders = {\n     'Authorization': 'Bearer ' + token,\n     'accept': 'application/json',\n     'Content-Type': 'application/json'\n}", "execution_count": 1, "outputs": []}, {"metadata": {"id": "72cbecfe9b384f3ebbe0c0ccf4b381e7"}, "cell_type": "markdown", "source": "# FILE MANIPULATION ROUTINES for STORAGE VOLUMES"}, {"metadata": {"id": "5183256607c144209f7b102b0929decc"}, "cell_type": "code", "source": "#\n# PROJECT FILE MANIPULATION\n#\n# Project file manipulation is done through use of the ibm-watson-studio-lib package\n# for Python.  You can read more abvout this package at:\n#  https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=lib-watson-studio-python\n#\n# Some notes:\n#  - you cannot delete files.  You can delete them from the filesystem, but then the\n#    file handle is still registered with the project area, and no file exists.  If you\n#    see corrupted files in project spaces, this is probably the cause.\n#  - you can doa raw os copy of a project file, but unless you then register the file,\n#    you will not see it in any other view of project files.\n#\n# Get data connections for this project\n#\ndef get_project_connections():\n    return_list = []\n    connection_list = wslib.list_connections()\n    for connection in connection_list:\n        inst_name = connection[\"name\"]\n        return_list.append(inst_name)\n    return return_list\n#\n# Get path to the files in the project\n#\ndef get_data_asset_path():\n    return wslib.mount.get_base_dir()\n#\n# Get a json object of dir of the files on the project space\n#\ndef get_proj_dir_json():\n    file_json_1 = wslib.list_connected_data()\n    file_json_2 = wslib.list_stored_data()\n    final_file_json = file_json_1 + file_json_2\n    return final_file_json\n#\n# Get a list of names of dir of the files on the project space\n#\ndef get_proj_dir_list():\n    final_file_json = get_proj_dir_json()\n    return_list = []\n    for file_ent in final_file_json:\n        file_name = file_ent[\"name\"]\n        return_list.append(file_name)\n    return return_list\n#\n# Pretty print a dir of the files on the project space\n#\ndef print_proj_dir_list():\n    file_list_1 = wslib.list_connected_data()\n    file_list_2 = wslib.list_stored_data()\n    final_file_list = file_list_1 + file_list_2\n    wslib.show(final_file_list)\n    return True\n#\n# Rename a file in the project data area\n#\ndef copy_project_file(old_name, new_name):\n    try:\n        stat = [0]\n        #\n        # Move the file to the data assets folder in the\n        # mounted project storage\n        #\n        data_asset_folder = get_data_asset_path()\n        old_target_path = os.path.join(data_asset_folder, old_name)\n        target_path = os.path.join(data_asset_folder, new_name)\n        shutil.copy(old_target_path, target_path)\n    except:\n        print(\"**ERROR**\", \" in copy_project_file - move \")\n        print(\"**ERROR**\", sys.exc_info())\n        stat = [1]\n    try:\n        #\n        # Register the file as data asset\n        #\n        wslib.mount.register_asset(target_path, asset_name=new_name)    \n    except:\n        print(\"**ERROR**\", \" in copy_project_file - register_asset\")\n        print(\"**ERROR**\", sys.exc_info())\n        stat = [1]\n    return stat\n#\n# Create a CSV file from a panda dataframe in a project data file\n# name = name of the resulting csv file (with no source directory)\n#\ndef write_pandas_df_to_proj_csv_file(name,dataframe):\n    try:\n        stat = wslib.save_data(name, str.encode(dataframe.to_csv(index=False)))\n    except:\n        print(\"**ERROR**\", \" in write_pandas_df_to_proj_csv_file \")\n        print(\"**ERROR**\", sys.exc_info())\n        stat = []\n    return stat", "execution_count": 2, "outputs": []}, {"metadata": {"id": "10cc90061fd643008027c9aad0092096"}, "cell_type": "code", "source": "#\n# STORAGE VOLUME FILE MANIPULATION\n#\n#\n# Storage volume file manipulation is done through use of the Volumes API.\n# You can read more abvout this API at:\n#  https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=resources-volumes-api\n#\n#\n# Get a list of the available storage volume names\n#\ndef get_storage_volume_list():\n    #\n    # Get a storage volumes and storage volume IDs\n    #\n    return_list = []\n    this_url = host + \"/zen-data/v3/service_instances?addon_type=volumes\"\n    dataDict = {\n        \"addon_type\":\"volumes\"\n        }\n    mydata = json.dumps(dataDict)\n    #\n    try:\n        r = requests.get(this_url,\n                         headers=headers,\n                         verify=False)\n    except:\n        print(\"**ERROR**\", \" in get_storage_volume_list \")\n        print(\"**ERROR**\", sys.exc_info())\n        return_list = []\n    #\n    storage_volumes = r.json()\n    #\n    instance_list = storage_volumes[\"service_instances\"]\n    for volume in instance_list:\n        inst_name = volume[\"display_name\"]\n        inst_id = volume[\"id\"]\n        mount_point = volume[\"parameters\"][\"mount_path\"]\n        return_list.append(inst_name)\n    #\n    return return_list \n#\n# Create a Volume API specific HTTP header with access token\n#\ndef get_volume_header():\n    #\n    # Get a storage volume token\n    #\n    this_url = host + \"/zen-data/v2/serviceInstance/token\"\n    dataDict = {\n        \"serviceInstanceID\": inst_id\n        }\n    mydata = json.dumps(dataDict)\n    #\n    try:\n        r = requests.get(this_url,\n                         headers=headers,\n                         data=mydata,\n                         verify=False)\n    except:\n        print(\"**ERROR**\", \" in get_volume_header \")\n        print(\"**ERROR**\", sys.exc_info())\n        volume_header = {\"error\":sys.exc_info()}\n    #\n    result = r.json()\n    volume_header = {\n         'Authorization': 'Bearer ' + result[\"AccessToken\"],\n         'Content-Type': 'application/json'\n        }\n    return volume_header\n#\n# Input the name of the storage volume, and discover the mount point\n#\ndef get_sv_mount_point(sv_name):\n    #\n    # Get a storage volumes and storage volume IDs\n    #\n    this_url = host + \"/zen-data/v3/service_instances?addon_type=volumes\"\n    dataDict = {\n        \"addon_type\":\"volumes\"\n        }\n    mydata = json.dumps(dataDict)\n    #\n    try:\n        r = requests.get(this_url,\n                         headers=headers,\n                         verify=False)\n    except:\n        print(\"**ERROR**\", \" in get_sv_mount_point \")\n        print(\"**ERROR**\", sys.exc_info())\n        ret_mtpt = \"error\"\n    #\n    storage_volumes = r.json()\n    #\n    ret_mntpt = \"\"\n    #\n    instance_list = storage_volumes[\"service_instances\"]\n    for volume in instance_list:\n        inst_name = volume[\"display_name\"]\n        inst_id = volume[\"id\"]\n        mount_point = volume[\"parameters\"][\"mount_path\"]\n        if (sv_name == inst_name):\n            ret_mtpt = mount_point\n    return ret_mtpt\n#\n# Get a list of files located at a specific directory path, for the \n# storage volume\n#\ndef get_sv_dir(sv_name,dir_path=\"\"):\n    # sv_name is a string - service volumen name\n    # dir_path is a string - target directory path to read\n    file_list = []\n    #\n    # Make sure that sstorage volume is mounted\n    #\n    this_url = host + \"/zen-data/v1/volumes/directories/volume_services\" + sv_name\n    dataDict = { }\n    mydata = json.dumps(dataDict)\n    #\n    try:\n        r = requests.get(this_url,\n                         headers=headers,\n                         data=mydata,\n                         verify=False)\n    except:\n        print(\"**ERROR**\", \" in get_sv_dir \")\n        print(\"**ERROR**\", sys.exc_info())\n        file_list = [\"error\", sys.exc_info()]\n    #\n    result = r.json()\n    #\n    file_path = get_sv_mount_point(sv_name) + dir_path\n    file_list = get_temp_dir_list(file_path)\n    return file_list\n#\n# Split_filename returns the filename and path from a full filename\n#\ndef split_filename(filename):\n    #\n    filebase = filename.split('/')[-1].split('.')[0]\n    return filebase\n#\n# Split_fileext returns the extension from a full filename\n#\ndef split_fileext(filename):\n    #\n    tempfilename, file_extension = os.path.splitext(filename)\n    while '.' in tempfilename:\n        tempfilename, tempfile_extension = os.path.splitext(tempfilename)\n        file_extension = tempfile_extension + file_extension       \n    return file_extension\n#\n# On a particular storage volume, at a particular path, backkup a file\n# by extending the filename with a string\n# (i.e. toxfile.txt becomes toxfile_20220310.txt)\n#\ndef backup_sv_file(sv_name,filename,dir_path=\"\",extension=\"\"):\n    #\n    stat=0\n    #\n    # if extension is defaulted, just add BACKUP\n    #\n    if extension == \"\":\n        extension = \"BACKUP\"\n    #\n    # Find your mount point, directory path, and filename\n    #\n    mount_pt = get_sv_mount_point(sv_name) + \"/\"\n    #\n    file_base = split_filename(filename)\n    file_ext = split_fileext(filename)\n    new_name = mount_pt + dir_path + file_base + extension + file_ext\n    #\n    old_name = mount_pt + dir_path + filename\n    #\n    print(\"Old file - \" + old_name + \"    - New Name - \" + new_name)\n    #\n    try:\n        stat = copy_temp_file(old_name, new_name)\n    except:\n        print(\"**ERROR**\", \" in backup_sv_file \")\n        print(\"**ERROR**\", sys.exc_info())\n        stat = []\n    return stat\n", "execution_count": 3, "outputs": []}, {"metadata": {"id": "d349314773cb459d8ba46a0fceba6c03"}, "cell_type": "code", "source": "#\n# TEMP DATA AREA FILE MANIPULATION\n#\n# Every CloudPak for Data Python notebook is run in a virtul environment, \n# and has it's own limited virtual file space.  this space can be used to\n# store temporary files and scratch files.\n#\n# Note that all of these routines are using standard Python utilities to\n# do file manipulation\n#\n# Get a dir of the files on the temp space\n#\ndef get_temp_dir_list(path=\".\"):\n    try:\n        final_file_list = os.listdir(path)\n    except:\n        print(\"**ERROR**\", \" in get_temp_dir_list \")\n        print(\"**ERROR**\", sys.exc_info())\n        final_file_list = []        \n    return final_file_list\n#\n# Rename a file in the temp data area\n#\ndef rename_temp_file(old_name, new_name):\n    try:\n        stat = os.rename(old_name,new_name) \n#        stat = shutil.move(old_name,new_name) \n    except:\n        print(\"**ERROR**\", \" in rename_temp_file \")\n        print(\"**ERROR**\", sys.exc_info())\n        stat = []\n    return stat\n#\n# Rename a file in the temp data area\n#\ndef copy_temp_file(old_name, new_name):\n    try:\n        stat = shutil.copy(old_name,new_name) \n    except:\n        print(\"**ERROR**\", \" in copy_temp_file \")\n        print(\"**ERROR**\", sys.exc_info())\n        stat = []\n    return stat\n#\n# Create a CSV file from a panda dataframe in a temp data file\n# name = name of the resulting csv file (with no source directory)\n#\ndef write_pandas_df_to_temp_csv_file(name,dataframe):\n    try:\n        stat = dataframe.to_csv(name,index=False)\n    except:\n        print(\"**ERROR**\", \" in write_pandas_df_to_temp_csv_file \")\n        print(\"**ERROR**\", sys.exc_info())\n        stat = []\n    return stat", "execution_count": 4, "outputs": []}, {"metadata": {"id": "bb0f7db89d7b464a8ff02bd4dfba28bd"}, "cell_type": "markdown", "source": "## Test and show sample code for the File manipulation routines"}, {"metadata": {"id": "4d401679154c46da9e56aa7ae895a5a4"}, "cell_type": "code", "source": "#\n#  PROJECT AREA EXAMPLES\n#\nprint (\"\\nPROJECT AREA EXAMPLES \\n\")\n#\n# List your connections\n#\nmy_connections = get_project_connections()\nprint(\"\\nMy connections:\\n\", my_connections)\n\n#\n# Find out where your data asset folder is located\n#\ndata_asset_path = get_data_asset_path()\nprint(\"\\nPath to data asset folder is:\\n\", data_asset_path)\n\n#\n# PROJECT DATA FILE EXAMPLES\n#\n# Get a directory of the files out on the project\n#\nmy_dir = get_proj_dir_json()\n#\nlist_dir = get_proj_dir_list()\nprint(\"\\nOriginal List of Files is:\\n\")\nprint(list_dir)\n\n#\n# Write out a sample data file to the project\n#\nSAMPLE_DATAFILE     = \"DUMMY_FILE.csv\"\nNEW_SAMPLE_DATAFILE = \"RENAMED_DUMMY_FILE.csv\"\n#\n# Dump data from dataframe to csv file\n#\nmy_dir_df = pd.DataFrame(data=my_dir)\ndata_file_info = write_pandas_df_to_proj_csv_file(SAMPLE_DATAFILE,my_dir_df)\nprint (\"\\nCreate new DataFile Info:\\n\")\nprint (data_file_info)\n#\nlist_dir = get_proj_dir_list()\nprint(\"\\nNew List of Files is:  (should include \" + SAMPLE_DATAFILE + \") \\n\")\nprint(list_dir)\n\n#\n# Rename the data file that you just created\n#\nprint(\"\\nCopying \" + SAMPLE_DATAFILE + \" to \" + NEW_SAMPLE_DATAFILE + \" \\n\")\nstat = copy_project_file(SAMPLE_DATAFILE,NEW_SAMPLE_DATAFILE)\n#\nlist_dir = get_proj_dir_list()\nprint(\"\\nNew List of Files is:\\n\")\nprint(list_dir)\n", "execution_count": 5, "outputs": [{"name": "stdout", "text": "\nPROJECT AREA EXAMPLES \n\n\nMy connections:\n ['CargoCodesSV']\n\nPath to data asset folder is:\n /project_data/data_asset/\n\nOriginal List of Files is:\n\n['CargoCodesInputFile', 'new_file', \"Today's Run\", 'DUMMY_FILE.csv', 'RENAMED_DUMMY_FILE.csv', 'Piece_Item_Data2021110107.csv', 'Novweek1_mismatch_list.csv', 'Piece_Item_Data2021110107.json', 'output_mismatch_list.csv', 'input_data_deployment.csv']\n**ERROR**  in write_pandas_df_to_proj_csv_file \n**ERROR** (<class 'RuntimeError'>, RuntimeError('Refusing to overwrite asset. (977148ab-6e98-4d5c-90ec-d1ddf4c0d0ce)'), <traceback object at 0x7f37a34fa640>)\n\nCreate new DataFile Info:\n\n[]\n\nNew List of Files is:  (should include DUMMY_FILE.csv) \n\n['CargoCodesInputFile', 'new_file', \"Today's Run\", 'DUMMY_FILE.csv', 'RENAMED_DUMMY_FILE.csv', 'Piece_Item_Data2021110107.csv', 'Novweek1_mismatch_list.csv', 'Piece_Item_Data2021110107.json', 'output_mismatch_list.csv', 'input_data_deployment.csv']\n\nCopying DUMMY_FILE.csv to RENAMED_DUMMY_FILE.csv \n\n**ERROR**  in copy_project_file - register_asset\n**ERROR** (<class 'RuntimeError'>, RuntimeError(\"There already exists an asset with name 'RENAMED_DUMMY_FILE.csv'. Specify a different asset name.\"), <traceback object at 0x7f37a34ddec0>)\n\nNew List of Files is:\n\n['CargoCodesInputFile', 'new_file', \"Today's Run\", 'DUMMY_FILE.csv', 'RENAMED_DUMMY_FILE.csv', 'Piece_Item_Data2021110107.csv', 'Novweek1_mismatch_list.csv', 'Piece_Item_Data2021110107.json', 'output_mismatch_list.csv', 'input_data_deployment.csv']\n", "output_type": "stream"}]}, {"metadata": {"id": "6569d32b5c51451f8c0bb12129320b61"}, "cell_type": "code", "source": "#\n#  TEMP AREA EXAMPLES\n#\nprint (\"\\nTEMP AREA EXAMPLES \\n\\n\")\n#\ntemp_dir = get_temp_dir_list()\nprint (\"\\nTemp dir is:\\n\")\nprint (temp_dir)\n#\n# Write out a sample data file to the temp space\n#\ndata_file_info = write_pandas_df_to_temp_csv_file(SAMPLE_DATAFILE,my_dir_df)\nprint (\"\\n\\nDataFile Info:\\n\")\nprint (data_file_info)\n#\ntemp_dir = get_temp_dir_list()\nprint (\"\\nTemp dir is:\\n\")\nprint (temp_dir)\n#\n# Rename the data file that you just created\n#\n# Note: when doing these manipulations, you WILL OVERWRITE existing files\n#\nstat = rename_temp_file(SAMPLE_DATAFILE,NEW_SAMPLE_DATAFILE)\n#\ntemp_dir = get_temp_dir_list()\nprint (\"\\nTemp dir is:\\n\")\nprint (temp_dir)\n\n\n\n#\n# Delete the file that you just created\n#\n\n#\n# List all of the connected data\n#\n", "execution_count": 6, "outputs": [{"name": "stdout", "text": "\nTEMP AREA EXAMPLES \n\n\n\nTemp dir is:\n\n['.virtual_documents', 'RENAMED_DUMMY_FILE.csv']\n\n\nDataFile Info:\n\nNone\n\nTemp dir is:\n\n['.virtual_documents', 'RENAMED_DUMMY_FILE.csv', 'DUMMY_FILE.csv']\n\nTemp dir is:\n\n['.virtual_documents', 'RENAMED_DUMMY_FILE.csv']\n", "output_type": "stream"}]}, {"metadata": {"id": "a37b4f8da95341d191220d6b90fc5ea9"}, "cell_type": "code", "source": "#\n# If you need to do RAW CURL calls then uncomment out this entire =block of code below\n#\n#import subprocess\n#SPACE = \" \"\n###\n### =============================================\n###\n###\n### Build your curl call\n###\n#curl_head = \"curl -k -X GET\"\n#url = \"https://cpd-cpd-instance.apps.poc.watson.techops.aa.com/zen-data/v3/service_instances?addon_type=volumes\"\n#header_line = \"-H 'Authorization: Bearer \" + token + \"' -H 'Content-Type: application/json'\"\n#data_line = \"\"\n#curl_cmd = curl_head + SPACE + url + SPACE + header_line + SPACE + data_line\n#print (\"===\\n\",curl_cmd,\"\\n===\\n\")\n#result = subprocess.run(curl_cmd, shell=True, capture_output=True)\n#result2 = result.stdout.decode()\n#print (result2)\n", "execution_count": 7, "outputs": []}, {"metadata": {"id": "85f674c8baa64612a23396e0541b3ac0"}, "cell_type": "code", "source": "# ==========================================\n#\n#  STORAGE VOLUME EXAMPLES\n#\nprint (\"\\nSTORAGE VOLUME EXAMPLES \\n\\n\")\n#\nimport time\nimport datetime\nfrom datetime import datetime, date, time, timezone\n###\n### =============================================\n###\ndef today_str():\n    today = datetime.now()\n    retstr = today.strftime(\"%d-%m-%Y\")\n    retstr = \"-\" + retstr\n    return retstr\n### =============================================\n###\n#\n# Get the list of available storage volumes\n#\nprint (\"\\nStorage volume list is: \")\nstor_vol_list = get_storage_volume_list()\nprint (stor_vol_list)\n#\ninst_name = stor_vol_list[0]\nprint(\"Focused on instance \"+inst_name)\n#\n# Get a directory\n#\nprint (\"\\nDirectory list is: \")\ndir_list = get_sv_dir(inst_name)\nprint (dir_list)\n#\n# Get a directory\n#\nprint (\"\\nDirectory list is: \")\ndir_list = get_sv_dir(inst_name,\"/.\")\nprint (dir_list)\n#\n# Get a directory\n#\nprint (\"\\nDirectory list (for bad directory path) is: \")\ndir_list = get_sv_dir(inst_name,\"some_wrong_dir\")\nprint (dir_list)\n#\n# Get a directory\n#\nprint (\"\\nDirectory list is: \")\ndir_list = get_sv_dir(inst_name,\"\")\nprint (dir_list)\n#\n# Backup/Copy the first file\n#\ntarget_file = dir_list[0]\n#\n# Backup a file\n#\nprint (\"\\n\\nBacking up file \" + target_file)\n#\n# Calculate your suffix for backup file extensiuon\n#\ndatestamp = today_str()\n#\n# Backup file\n#\nstat = backup_sv_file(inst_name,target_file,\"\",datestamp)\n#\n# Get a directory\n#\nprint (\"\\nDirectory list is: \")\ndir_list = get_sv_dir(inst_name,\"\")\nprint (dir_list)\n", "execution_count": 8, "outputs": [{"name": "stdout", "text": "\nSTORAGE VOLUME EXAMPLES \n\n\n\nStorage volume list is: \n['CargoCodesSV']\nFocused on instance CargoCodesSV\n\nDirectory list is: \n['Piece_Item_Data2021110107.csv', 'Com_code_mismatch_today.csv', 'Cargo_Mismatch_output_today.csv', 'Piece_Item_Data2021110107-11-03-2022.csv', 'Piece_Item_Data2021110107-14-03-2022.csv', 'Piece_Item_Data2021110107-16-03-2022.csv', 'Piece_Item_Data2021110107-17-03-2022.csv']\n\nDirectory list is: \n['Piece_Item_Data2021110107.csv', 'Com_code_mismatch_today.csv', 'Cargo_Mismatch_output_today.csv', 'Piece_Item_Data2021110107-11-03-2022.csv', 'Piece_Item_Data2021110107-14-03-2022.csv', 'Piece_Item_Data2021110107-16-03-2022.csv', 'Piece_Item_Data2021110107-17-03-2022.csv']\n\nDirectory list (for bad directory path) is: \n**ERROR**  in get_temp_dir_list \n**ERROR** (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f37d84e8840>)\n[]\n\nDirectory list is: \n['Piece_Item_Data2021110107.csv', 'Com_code_mismatch_today.csv', 'Cargo_Mismatch_output_today.csv', 'Piece_Item_Data2021110107-11-03-2022.csv', 'Piece_Item_Data2021110107-14-03-2022.csv', 'Piece_Item_Data2021110107-16-03-2022.csv', 'Piece_Item_Data2021110107-17-03-2022.csv']\n\n\nBacking up file Piece_Item_Data2021110107.csv\nOld file - /mnts/CargoCodesSV/Piece_Item_Data2021110107.csv    - New Name - /mnts/CargoCodesSV/Piece_Item_Data2021110107-17-03-2022.csv\n\nDirectory list is: \n['Piece_Item_Data2021110107.csv', 'Com_code_mismatch_today.csv', 'Cargo_Mismatch_output_today.csv', 'Piece_Item_Data2021110107-11-03-2022.csv', 'Piece_Item_Data2021110107-14-03-2022.csv', 'Piece_Item_Data2021110107-16-03-2022.csv', 'Piece_Item_Data2021110107-17-03-2022.csv']\n", "output_type": "stream"}]}, {"metadata": {"id": "61100a3ce7c448908f4c280f5eb98a24"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.7", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}